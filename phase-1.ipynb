{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\.conda\\envs\\r-conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import json\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gradio as grad\n",
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=100)\n",
    "\n",
    "\n",
    "def get_arxiv_papers(query: str, max_papers: int = 5) -> list[dict]:\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(query=query, max_results = max_papers)\n",
    "    papers = []\n",
    "\n",
    "    for result in client.results(search):\n",
    "        paper = {\n",
    "            \"title\" : result.title,\n",
    "            \"authors\": [a.name for a in result.authors],\n",
    "            \"summary\": result.summary,\n",
    "            \"published\": str(result.published.date()),\n",
    "            \"url\": result.pdf_url\n",
    "        }\n",
    "        papers.append(paper)\n",
    "\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt\n",
    "\n",
    "@retry(stop=stop_after_attempt(5))\n",
    "\n",
    "\n",
    "def get_ieee_papers(query:str, api_key:str, max_results:int = 5) -> list[dict]:\n",
    "    url = \"https://ieeexploreapi.ieee.org/api/v1/search/articles\"\n",
    "    params = {\n",
    "        \"querytext\": query,\n",
    "        'apikey': api_key,\n",
    "        'max_records': max_results,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        papers = []\n",
    "        \n",
    "        for article in response.json().get('articles', []):\n",
    "            papers.append({\n",
    "                \"title\": article.get('title', 'no title'),\n",
    "                \"authors\": [author[\"full_name\"] for author in article.get('authors', [])],\n",
    "                \"abstract\":article.get('abstract', 'No abstract'),\n",
    "                'published': article.get('publication_date',\"\"),\n",
    "                'url': article.get('pdf_url', article.get('html_url',''))\n",
    "            })\n",
    "        return papers\n",
    "    except Exception as e:\n",
    "        raise Exception(f'IEEE API request failed: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_agent(query:str, ieee_api_key:str, max_papers: int = 5)-> list[str]:\n",
    "    try:\n",
    "        papers = get_arxiv_papers(query, max_papers)\n",
    "        if papers:\n",
    "            return papers\n",
    "    except Exception as e:\n",
    "        print(f'arXiv failed: {str(e)}')\n",
    "        \n",
    "    if ieee_api_key:\n",
    "        try:\n",
    "            return get_ieee_papers(query, ieee_api_key, max_papers)\n",
    "        except Exception as e:\n",
    "            print(f'IEEE failed: {str(e)}')\n",
    "    raise Exception(\"something's wrong with API connection\")        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(papers: list[dict]):\n",
    "    for i, paper in enumerate(papers,1):\n",
    "        print(f\"\"\"\n",
    "        [{i}] {paper['title']}\n",
    "        Authors:{''.join(paper.get('authors', []))}\n",
    "        Published: {paper.get('published', 'N/A')}\n",
    "        Summary: {paper.get('summary', paper.get('abstract', 'N/A'))[:500]}...\n",
    "        URL:{paper.get('url','N/A')}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=LLM+quantization&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "INFO:arxiv:Got first page: 100 of 70131 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        [1] A Comprehensive Evaluation of Quantization Strategies for Large Language Models\n",
      "        Authors:Renren JinJiangcun DuWuwei HuangWei LiuJian LuanBin WangDeyi Xiong\n",
      "        Published: 2024-02-26\n",
      "        Summary: Increasing the number of parameters in large language models (LLMs) usually\n",
      "improves performance in downstream tasks but raises compute and memory costs,\n",
      "making deployment difficult in resource-limited settings. Quantization\n",
      "techniques, which reduce the bits needed for model weights or activations with\n",
      "minimal performance loss, have become popular due to the rise of LLMs. However,\n",
      "most quantization studies use pre-trained LLMs, and the impact of quantization\n",
      "on instruction-tuned LLMs and the rel...\n",
      "        URL:http://arxiv.org/pdf/2402.16775v2\n",
      "        \n",
      "\n",
      "        [2] Achieving binary weight and activation for LLMs using Post-Training Quantization\n",
      "        Authors:Siqing SongChuang WangRuiqi WangYi YangXuyao Zhang\n",
      "        Published: 2025-04-07\n",
      "        Summary: Quantizing large language models (LLMs) to 1-bit precision significantly\n",
      "reduces computational costs, but existing quantization techniques suffer from\n",
      "noticeable performance degradation when using weight and activation precisions\n",
      "below 4 bits (W4A4). In this paper, we propose a post-training quantization\n",
      "framework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit\n",
      "with an additional 1 bit for fine-grain grouping and activations are quantized\n",
      "to 1 bit with a 4-fold increase in ...\n",
      "        URL:http://arxiv.org/pdf/2504.05352v2\n",
      "        \n",
      "\n",
      "        [3] CrossQuant: A Post-Training Quantization Method with Smaller Quantization Kernel for Precise Large Language Model Compression\n",
      "        Authors:Wenyuan LiuXindian MaPeng ZhangYan Wang\n",
      "        Published: 2024-10-10\n",
      "        Summary: Post-Training Quantization (PTQ) is an effective technique for compressing\n",
      "Large Language Models (LLMs). While many studies focus on quantizing both\n",
      "weights and activations, it is still a challenge to maintain the accuracy of\n",
      "LLM after activating quantization. To investigate the primary cause, we extend\n",
      "the concept of kernel from linear algebra to quantization functions to define a\n",
      "new term, \"quantization kernel\", which refers to the set of elements in\n",
      "activations that are quantized to zero. Thr...\n",
      "        URL:http://arxiv.org/pdf/2410.07505v1\n",
      "        \n",
      "\n",
      "        [4] The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs\n",
      "        Authors:Mert YazanSuzan VerberneFrederik Situmeang\n",
      "        Published: 2024-06-10\n",
      "        Summary: Post-training quantization reduces the computational demand of Large Language\n",
      "Models (LLMs) but can weaken some of their capabilities. Since LLM abilities\n",
      "emerge with scale, smaller LLMs are more sensitive to quantization. In this\n",
      "paper, we explore how quantization affects smaller LLMs' ability to perform\n",
      "retrieval-augmented generation (RAG), specifically in longer contexts. We chose\n",
      "personalization for evaluation because it is a challenging domain to perform\n",
      "using RAG as it requires long-contex...\n",
      "        URL:http://arxiv.org/pdf/2406.10251v3\n",
      "        \n",
      "\n",
      "        [5] What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation\n",
      "        Authors:Zhuocheng GongJiahao LiuJingang WangXunliang CaiDongyan ZhaoRui Yan\n",
      "        Published: 2024-03-11\n",
      "        Summary: Quantization has emerged as a promising technique for improving the memory\n",
      "and computational efficiency of large language models (LLMs). Though the\n",
      "trade-off between performance and efficiency is well-known, there is still much\n",
      "to be learned about the relationship between quantization and LLM performance.\n",
      "To shed light on this relationship, we propose a new perspective on\n",
      "quantization, viewing it as perturbations added to the weights and activations\n",
      "of LLMs. We call this approach \"the lens of pe...\n",
      "        URL:http://arxiv.org/pdf/2403.06408v1\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "papers = research_agent('LLM quantization', ieee_api_key=\"#\")\n",
    "get_results(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection('research_papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_papers(papers: list[dict]):\n",
    "    ids = [paper['url'] for paper in papers]\n",
    "    documents = [paper['title']+ '\\n' + paper['summary'] for paper in papers]\n",
    "    embeddings = model.encode(documents).tolist()\n",
    "\n",
    "    metadatas = []\n",
    "    for paper in papers:\n",
    "        metadata = paper.copy()\n",
    "        metadata['authors'] = ','.join(paper['authors'])\n",
    "        metadata['published'] = str(paper['published'])\n",
    "        metadatas.append(metadata)\n",
    "\n",
    "    collection.add(\n",
    "        ids = ids,\n",
    "        embeddings = embeddings,\n",
    "        documents = documents,\n",
    "        metadatas = metadatas\n",
    "    )\n",
    "\n",
    "def similar_papers(query:str, n_results:int = 5) -> list[dict]:\n",
    "    results = collection.query(\n",
    "        query_embeddings = model.encode(query).tolist(),\n",
    "        n_results = n_results\n",
    "    )\n",
    "    return results['metadatas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Quant+Finance&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "INFO:arxiv:Requesting page (first: True, try: 1): https://export.arxiv.org/api/query?search_query=Quant+Finance&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "INFO:arxiv:Got first page: 100 of 30847 total results\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        [1] Quant 4.0: Engineering Quantitative Investment with Automated, Explainable and Knowledge-driven Artificial Intelligence\n",
      "        Authors:Jian Guo,Saizhuo Wang,Lionel M. Ni,Heung-Yeung Shum\n",
      "        Published: 2022-12-13\n",
      "        Summary: Quantitative investment (``quant'') is an interdisciplinary field combining\n",
      "financial engineering, computer science, mathematics, statistics, etc. Quant\n",
      "has become one of the mainstream investment methodologies over the past\n",
      "decades, and has experienced three generations: Quant 1.0, trading by\n",
      "mathematical modeling to discover mis-priced assets in markets; Quant 2.0,\n",
      "shifting quant research pipeline from small ``strategy workshops'' to large\n",
      "``alpha factories''; Quant 3.0, applying deep learning...\n",
      "        URL:http://arxiv.org/pdf/2301.04020v1\n",
      "        \n",
      "\n",
      "        [2] Quant GANs: Deep Generation of Financial Time Series\n",
      "        Authors:Magnus Wiese,Robert Knobloch,Ralf Korn,Peter Kretschmer\n",
      "        Published: 2019-07-15\n",
      "        Summary: Modeling financial time series by stochastic processes is a challenging task\n",
      "and a central area of research in financial mathematics. As an alternative, we\n",
      "introduce Quant GANs, a data-driven model which is inspired by the recent\n",
      "success of generative adversarial networks (GANs). Quant GANs consist of a\n",
      "generator and discriminator function, which utilize temporal convolutional\n",
      "networks (TCNs) and thereby achieve to capture long-range dependencies such as\n",
      "the presence of volatility clusters. The ...\n",
      "        URL:http://arxiv.org/pdf/1907.06673v2\n",
      "        \n",
      "\n",
      "        [3] From Deep Learning to LLMs: A survey of AI in Quantitative Investment\n",
      "        Authors:Bokai Cao,Saizhuo Wang,Xinyi Lin,Xiaojun Wu,Haohan Zhang,Lionel M. Ni,Jian Guo\n",
      "        Published: 2025-03-27\n",
      "        Summary: Quantitative investment (quant) is an emerging, technology-driven approach in\n",
      "asset management, increasingy shaped by advancements in artificial\n",
      "intelligence. Recent advances in deep learning and large language models (LLMs)\n",
      "for quant finance have improved predictive modeling and enabled agent-based\n",
      "automation, suggesting a potential paradigm shift in this field. In this\n",
      "survey, taking alpha strategy as a representative example, we explore how AI\n",
      "contributes to the quantitative investment pipeli...\n",
      "        URL:http://arxiv.org/pdf/2503.21422v1\n",
      "        \n",
      "\n",
      "        [4] The US 2000-2003 Market Descent: Clarifications\n",
      "        Authors:D. Sornette,W. -X. Zhou\n",
      "        Published: 2003-04-30\n",
      "        Summary: In a recent comment (Johansen A 2003 An alternative view, Quant. Finance 3:\n",
      "C6-C7, cond-mat/0302141), Anders Johansen has criticized our methodology and\n",
      "has questioned several of our results published in [Sornette D and Zhou W-X\n",
      "2002 The US 2000-2002 market descent: how much longer and deeper? Quant.\n",
      "Finance 2: 468-81, cond-mat/0209065] and in our two consequent preprints\n",
      "[cond-mat/0212010, physics/0301023]. In the present reply, we clarify the\n",
      "issues on (i) the analogy between rupture and crash...\n",
      "        URL:http://arxiv.org/pdf/cond-mat/0305004v1\n",
      "        \n",
      "\n",
      "        [5] Comment on recent claims by Sornette and Zhou\n",
      "        Authors:Anders Johansen\n",
      "        Published: 2003-02-07\n",
      "        Summary: Comment on recent claims by Sornette and Zhou: D. Sornette and W. Zhou,\n",
      "Quantitative Finance 2 (6), 468-481 (2002); Evidence of a Worldwide Stock\n",
      "Market Log-Periodic Anti-Bubble Since Mid-2000, cond-mat/0212010;\n",
      "Renormalization Group Analysis of the 2000-2002 anti-bubble in the US SP 500\n",
      "index, physics/0301023...\n",
      "        URL:http://arxiv.org/pdf/cond-mat/0302141v1\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "papers = research_agent('Quant Finance', ieee_api_key='#')\n",
    "\n",
    "store_papers(papers)\n",
    "\n",
    "relevant_papers = similar_papers('Price Prediction techniques')\n",
    "\n",
    "get_results(relevant_papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://a0c99e0bc6b4bd3571.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://a0c99e0bc6b4bd3571.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a0c99e0bc6b4bd3571.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradio(query: str):\n",
    "    papers = research_agent(query, ieee_api_key='#')\n",
    "\n",
    "    store_papers(papers)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for i, paper in enumerate(papers,1):\n",
    "        output.append(\n",
    "            f'**[{i}] {paper['title']}**\\n'\n",
    "            f'Authors: {','.join(paper['authors'])}\\n'\n",
    "            f'Published: {paper['published']}\\n'\n",
    "            f'Summary: {paper['summary'][:500]}\\n'\n",
    "            f'[Read Paper]({paper['url']})\\n'\n",
    "        )\n",
    "\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n",
    "grad.Interface(\n",
    "    fn = gradio,\n",
    "    inputs = grad.Textbox(label=\"Research Topic\", placeholder=\"e.g., Explainable AI\"),\n",
    "    outputs=grad.Markdown(label='Results'),\n",
    "    title= \"Autonomous Research Agent\",\n",
    "    description=\"Fetch tech papers from arXiv with semantic search\"\n",
    ").launch(share= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gemini connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"#\")\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest(papers):\n",
    "    prompt = \"\"\"You are a helpful AI assistant that helps researchers find good project topics.\n",
    "Below are the titles and abstracts of recent research papers. Based on these, generate 3–5 new, actionable, and specific research ideas.\n",
    "\n",
    "For each idea, use this format:\n",
    "---\n",
    "Title: [Short title]\n",
    "Problem Statement: [What the research would explore]\n",
    "Motivation: [Why this problem matters]\n",
    "Suggested Approach: [Methodology / tools / techniques]\n",
    "Possible Datasets: [If any]\n",
    "Tags: [Domain, difficulty (Beginner/Intermediate/Advanced), novelty (High/Medium/Low)]\n",
    "Inspired by: [Which paper(s) this idea builds on]\n",
    "\n",
    "Research Papers:\n",
    "\"\"\"\n",
    "\n",
    "    for i, paper in enumerate(papers, 1):\n",
    "        prompt += f\"\\nPaper {i}:\\nTitle: {paper['title']}\\nAbstract: {paper['abstract']}\\n\"\n",
    "\n",
    "    prompt += \"\\nNow generate the ideas.\\n\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\.conda\\envs\\r-conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import json\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=100)\n",
    "\n",
    "\n",
    "def get_arxiv_papers(query: str, max_papers: int = 5) -> list[dict]:\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(query=query, max_results = max_papers)\n",
    "    papers = []\n",
    "\n",
    "    for result in client.results(search):\n",
    "        paper = {\n",
    "            \"title\" : result.title,\n",
    "            \"authors\": [a.name for a in result.authors],\n",
    "            \"summary\": result.summary,\n",
    "            \"published\": str(result.published.date()),\n",
    "            \"url\": result.pdf_url\n",
    "        }\n",
    "        papers.append(paper)\n",
    "\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt\n",
    "\n",
    "@retry(stop=stop_after_attempt(5))\n",
    "\n",
    "\n",
    "def get_ieee_papers(query:str, api_key:str, max_results:int = 5) -> list[dict]:\n",
    "    url = \"https://ieeexploreapi.ieee.org/api/v1/search/articles\"\n",
    "    params = {\n",
    "        \"querytext\": query,\n",
    "        'apikey': api_key,\n",
    "        'max_records': max_results,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        papers = []\n",
    "        \n",
    "        for article in response.json().get('articles', []):\n",
    "            papers.append({\n",
    "                \"title\": article.get('title', 'no title'),\n",
    "                \"authors\": [author[\"full_name\"] for author in article.get('authors', [])],\n",
    "                \"abstract\":article.get('abstract', 'No abstract'),\n",
    "                'published': article.get('publication_date',\"\"),\n",
    "                'url': article.get('pdf_url', article.get('html_url',''))\n",
    "            })\n",
    "        return papers\n",
    "    except Exception as e:\n",
    "        raise Exception(f'IEEE API request failed: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_agent(query:str, ieee_api_key:str, max_papers: int = 5)-> list[str]:\n",
    "    try:\n",
    "        papers = get_arxiv_papers(query, max_papers)\n",
    "        if papers:\n",
    "            return papers\n",
    "    except Exception as e:\n",
    "        print(f'arXiv failed: {str(e)}')\n",
    "        \n",
    "    #if ieee_api_key:\n",
    "        #try:\n",
    "            #return get_ieee_papers(query, ieee_api_key, max_papers)\n",
    "        #except Exception as e:\n",
    "            #print(f'IEEE failed: {str(e)}')\n",
    "    #raise Exception(\"something's wrong with API connection\")        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(papers: list[dict]):\n",
    "    for i, paper in enumerate(papers,1):\n",
    "        print(f\"\"\"\n",
    "        [{i}] {paper['title']}\n",
    "        Authors:{','.join(paper.get('authors', []))}\n",
    "        Published: {paper.get('published', 'N/A')}\n",
    "        Summary: {paper.get('summary', paper.get('abstract', 'N/A'))[:500]}...\n",
    "        URL:{paper.get('url','N/A')}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=LLM+quantization&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "INFO:arxiv:Got first page: 100 of 67384 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        [1] A Comprehensive Evaluation of Quantization Strategies for Large Language Models\n",
      "        Authors:Renren Jin,Jiangcun Du,Wuwei Huang,Wei Liu,Jian Luan,Bin Wang,Deyi Xiong\n",
      "        Published: 2024-02-26\n",
      "        Summary: Increasing the number of parameters in large language models (LLMs) usually\n",
      "improves performance in downstream tasks but raises compute and memory costs,\n",
      "making deployment difficult in resource-limited settings. Quantization\n",
      "techniques, which reduce the bits needed for model weights or activations with\n",
      "minimal performance loss, have become popular due to the rise of LLMs. However,\n",
      "most quantization studies use pre-trained LLMs, and the impact of quantization\n",
      "on instruction-tuned LLMs and the rel...\n",
      "        URL:http://arxiv.org/pdf/2402.16775v2\n",
      "        \n",
      "\n",
      "        [2] Achieving binary weight and activation for LLMs using Post-Training Quantization\n",
      "        Authors:Siqing Song,Chuang Wang,Ruiqi Wang,Yi Yang,Xuyao Zhang\n",
      "        Published: 2025-04-07\n",
      "        Summary: Quantizing large language models (LLMs) to 1-bit precision significantly\n",
      "reduces computational costs, but existing quantization techniques suffer from\n",
      "noticeable performance degradation when using weight and activation precisions\n",
      "below 4 bits (W4A4). In this paper, we propose a post-training quantization\n",
      "framework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit\n",
      "with an additional 1 bit for fine-grain grouping and activations are quantized\n",
      "to 1 bit with a 4-fold increase in ...\n",
      "        URL:http://arxiv.org/pdf/2504.05352v2\n",
      "        \n",
      "\n",
      "        [3] CrossQuant: A Post-Training Quantization Method with Smaller Quantization Kernel for Precise Large Language Model Compression\n",
      "        Authors:Wenyuan Liu,Xindian Ma,Peng Zhang,Yan Wang\n",
      "        Published: 2024-10-10\n",
      "        Summary: Post-Training Quantization (PTQ) is an effective technique for compressing\n",
      "Large Language Models (LLMs). While many studies focus on quantizing both\n",
      "weights and activations, it is still a challenge to maintain the accuracy of\n",
      "LLM after activating quantization. To investigate the primary cause, we extend\n",
      "the concept of kernel from linear algebra to quantization functions to define a\n",
      "new term, \"quantization kernel\", which refers to the set of elements in\n",
      "activations that are quantized to zero. Thr...\n",
      "        URL:http://arxiv.org/pdf/2410.07505v1\n",
      "        \n",
      "\n",
      "        [4] The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs\n",
      "        Authors:Mert Yazan,Suzan Verberne,Frederik Situmeang\n",
      "        Published: 2024-06-10\n",
      "        Summary: Post-training quantization reduces the computational demand of Large Language\n",
      "Models (LLMs) but can weaken some of their capabilities. Since LLM abilities\n",
      "emerge with scale, smaller LLMs are more sensitive to quantization. In this\n",
      "paper, we explore how quantization affects smaller LLMs' ability to perform\n",
      "retrieval-augmented generation (RAG), specifically in longer contexts. We chose\n",
      "personalization for evaluation because it is a challenging domain to perform\n",
      "using RAG as it requires long-contex...\n",
      "        URL:http://arxiv.org/pdf/2406.10251v3\n",
      "        \n",
      "\n",
      "        [5] What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation\n",
      "        Authors:Zhuocheng Gong,Jiahao Liu,Jingang Wang,Xunliang Cai,Dongyan Zhao,Rui Yan\n",
      "        Published: 2024-03-11\n",
      "        Summary: Quantization has emerged as a promising technique for improving the memory\n",
      "and computational efficiency of large language models (LLMs). Though the\n",
      "trade-off between performance and efficiency is well-known, there is still much\n",
      "to be learned about the relationship between quantization and LLM performance.\n",
      "To shed light on this relationship, we propose a new perspective on\n",
      "quantization, viewing it as perturbations added to the weights and activations\n",
      "of LLMs. We call this approach \"the lens of pe...\n",
      "        URL:http://arxiv.org/pdf/2403.06408v1\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "papers = research_agent('LLM quantization', ieee_api_key=\"#\")\n",
    "get_results(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection('research_papers')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
